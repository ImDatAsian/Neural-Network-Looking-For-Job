# -*- coding: utf-8 -*-
"""Neural Network Job

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zHKk2qg8RX3nTmdmxioAxvjF15O_0uiT
"""
# Importing files needed to construct neural network and clean data
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers

# Getting the data from the url
url = tf.keras.utils.get_file('aug_train.csv', 'https://raw.githubusercontent.com/ImDatAsian/Neural-Network-Looking-For-Job/main/aug_train.csv')

# Reading the file contents and storing them in a dataframe
df = pd.read_csv(url)

# Reading the data in the columns and making them into columns with one or zero as data in them
dum_rel_exp = pd.get_dummies(df['Relevent Experience'])
dum_en_un = pd.get_dummies(df['Enrolled University'])
dum_ed_lvl = pd.get_dummies(df['Education Level'])
dum_c_m = pd.get_dummies(df['College Major'])
dum_exp = pd.get_dummies(df['Experience'])
dum_c_s = pd.get_dummies(df['Company Size'])
dum_c_t = pd.get_dummies(df['Company Type'])
dum_l_n_t = pd.get_dummies(df['Last New Job'])

# Merging them into the dataframe
df = pd.merge(
    left=df,
    right=dum_rel_exp,
    left_index=True,
    right_index=True,
)
df = pd.merge(
    left=df,
    right=dum_en_un,
    left_index=True,
    right_index=True,
)
df = pd.merge(
    left=df,
    right=dum_ed_lvl,
    left_index=True,
    right_index=True,
)
df = pd.merge(
    left=df,
    right=dum_c_m,
    left_index=True,
    right_index=True,
)
df = pd.merge(
    left=df,
    right=dum_exp,
    left_index=True,
    right_index=True,
)
df = pd.merge(
    left=df,
    right=dum_c_s,
    left_index=True,
    right_index=True,
)
df = pd.merge(
    left=df,
    right=dum_c_t,
    left_index=True,
    right_index=True,
)
df = pd.merge(
    left=df,
    right=dum_l_n_t,
    left_index=True,
    right_index=True,
)

# Dropping the columns that are no longer needed
df = df.drop(columns=['Enrollee Id','Gender', 'City','Relevent Experience','Enrolled University','Education Level','College Major','Experience','Company Size','Company Type','Last New Job'])

# Turning the target column into its own dataframe
target = df.pop('Target')

# Dividing up the dataframe into three sets. Seventy percent for training, fifteen percent for vaildation, and fifteen percent for testing
train, val, test = np.split(df.sample(frac=1, random_state=1),
                            [int(.7*len(df)), int(.85*len(df))])

# Same deal as above but this is for the target dataframe
train_target, val_target, test_target = np.split(target.sample(frac=1, random_state=1),
                                                 [int(.7*len(df)), int(.85*len(df))])

# Turning the dataframe with their respective data into data subsets
train_dataset = tf.data.Dataset.from_tensor_slices((train.values, train_target.values))
validate_dataset = tf.data.Dataset.from_tensor_slices((val.values, val_target.values))
test_dataset = tf.data.Dataset.from_tensor_slices((test.values, test_target.values))

train_ds = train_dataset.shuffle(len(train)).batch(64) # Shuffle the training dataset and break up the data into batches of 64
val_ds = validate_dataset.batch(32) # Break up the validate dataset into batches of 32
test_ds = test_dataset.batch(128) # Break up the test dataset into batches of 128

# Contructing the model
model = tf.keras.Sequential([
                             layers.Dense(128, activation='linear'), # first hidden layer with linear activation and 128 neurons
                             layers.Dense(128, activation='linear'), # second hidden layer with linear activation and 128 neurons
                             layers.Dense(2, activation='softmax') # output layer with softmax activation and 2 neurons
])
model.compile(optimizer='adam', # Implements Adam algorithm 
              loss='sparse_categorical_crossentropy', # Implements sparse categorical crossentropy loss function 
              metrics=['accuracy']) # This will calculate how often predictions equal to labels


model.fit(train_ds, validation_data=val_ds, epochs=60) # training begins here with 60 epochs

model.evaluate(test_ds) # testing begins here

df.to_csv('aug_train_cleaned') # Outputs the csv file that is cleaned to a downloadable file
